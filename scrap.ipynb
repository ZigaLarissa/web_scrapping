{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping\n",
    "\n",
    "It is not always that we have access to a neat, organized dataset avaliable in the .csv format;   \n",
    "sometimes, the data we need may be available on the web, and we have to be capable of collecting it.   \n",
    "Luckily for us, Python has a solution in the form of the package Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the necessary libraries, we have to download the actual HTML of the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the page using requests\n",
    "\n",
    "# url\n",
    "url = \"https://pt.wikipedia.org/wiki/Lista_de_bairros_de_Manaus\"\n",
    "\n",
    "# get the url content\n",
    "response = requests.get(url)\n",
    "content = response.content\n",
    "\n",
    "# creating a beautiful soup object\n",
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Table\n",
    "\n",
    "We now have the HTML of the page, so we need to find the table we want. We could retrieve the first table available, but there is the possibility the page contains more than one table, which is common in Wikipedia pages. For this reason, we have to look at all tables and find the correct one. We cannot advance blindly, though. Let us have a look at the structure of the HTML.\n",
    "\n",
    "\n",
    " Unfortunately, the tables do not have a title, but they do have a class attribute. We can use this information to pick the correct table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes of each table: \n",
      "['box-Desatualizado', 'plainlinks', 'metadata', 'ambox', 'ambox-content']\n",
      "['wikitable', 'sortable']\n",
      "['nowraplinks', 'collapsible', 'collapsed', 'navbox-inner']\n"
     ]
    }
   ],
   "source": [
    "# verify tables and their classes\n",
    "print('Classes of each table: ')\n",
    "for table in soup.find_all('table'):\n",
    "    print(table.get('class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen the tables available, and the one we want is the second table (aka. class = ‘wikitable’ and ‘sortable’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of all tables\n",
    "tables = soup.find_all('table')\n",
    "\n",
    "# looking for our designated table\n",
    "table = soup.find('table', class_ = 'wikitable sortable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the correct data, we can extract its data to create our very own dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Zone</th>\n",
       "      <th>Area</th>\n",
       "      <th>Population</th>\n",
       "      <th>Density</th>\n",
       "      <th>Homes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adrianópolis</td>\n",
       "      <td>Centro-Sul</td>\n",
       "      <td>248.45</td>\n",
       "      <td>10459</td>\n",
       "      <td>3560.88</td>\n",
       "      <td>3224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aleixo</td>\n",
       "      <td>Centro-Sul</td>\n",
       "      <td>618.34</td>\n",
       "      <td>24417</td>\n",
       "      <td>3340.4</td>\n",
       "      <td>6101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alvorada</td>\n",
       "      <td>Centro-Oeste</td>\n",
       "      <td>553.18</td>\n",
       "      <td>76392</td>\n",
       "      <td>11681.73</td>\n",
       "      <td>18193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armando Mendes</td>\n",
       "      <td>Leste</td>\n",
       "      <td>307.65</td>\n",
       "      <td>33441</td>\n",
       "      <td>9194.86</td>\n",
       "      <td>7402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Betânia</td>\n",
       "      <td>Sul</td>\n",
       "      <td>52.51</td>\n",
       "      <td>1294</td>\n",
       "      <td>20845.55</td>\n",
       "      <td>3119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Neighborhood          Zone    Area Population   Density Homes_count\n",
       "0    Adrianópolis    Centro-Sul  248.45      10459   3560.88        3224\n",
       "1          Aleixo    Centro-Sul  618.34      24417    3340.4        6101\n",
       "2        Alvorada  Centro-Oeste  553.18      76392  11681.73       18193\n",
       "3  Armando Mendes         Leste  307.65      33441   9194.86        7402\n",
       "4         Betânia           Sul   52.51       1294  20845.55        3119"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# defining the columns' headers\n",
    "header = ['Neighborhood', 'Zone', 'Area', 'Population', 'Density', 'Homes_count']\n",
    "\n",
    "# Collecting the data\n",
    "rows = []\n",
    "for row in table.tbody.find_all('tr'):\n",
    "    # find all data for each column\n",
    "    columns = row.find_all('td')\n",
    "    \n",
    "    if columns != []:\n",
    "        neighborhood = columns[0].text.strip()\n",
    "        zone = columns[1].text.strip()\n",
    "        area = columns[2].span.contents[0].strip('&0.')\n",
    "        population = columns[3].span.contents[0].strip('&0.')\n",
    "        density = columns[4].span.contents[0].strip('&0.')\n",
    "        homes_count = columns[5].span.contents[0].strip('&0.')\n",
    "        \n",
    "        # append to rows\n",
    "        rows.append([neighborhood, zone, area, population, density, homes_count])\n",
    "\n",
    "# dataframe\n",
    "df = pd.DataFrame(data = rows, columns = header)\n",
    "\n",
    "# display the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A different url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
